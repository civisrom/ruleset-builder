#!/usr/bin/env python3
"""
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ruleset
–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è, –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ, –∞–Ω–∞–ª–∏–∑
"""

import json
import argparse
import os
from typing import Dict, List, Set
from collections import Counter

# ============================================================================
# –ö–û–ù–í–ï–†–¢–ê–¶–ò–Ø –§–û–†–ú–ê–¢–û–í
# ============================================================================

class FormatConverter:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ–∂–¥—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏"""
    
    @staticmethod
    def clash_to_singbox(clash_file: str, output_file: str):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è Clash YAML –≤ Sing-Box JSON"""
        try:
            import yaml
        except ImportError:
            print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ PyYAML: pip install pyyaml")
            return
        
        with open(clash_file, 'r', encoding='utf-8') as f:
            clash_data = yaml.safe_load(f)
        
        singbox_rules = []
        domain_rule = {}
        ip_rule = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ –∏–∑ Clash
        if 'rules' in clash_data:
            for rule in clash_data['rules']:
                parts = rule.split(',')
                if len(parts) < 2:
                    continue
                
                rule_type = parts[0]
                value = parts[1]
                
                if rule_type == 'DOMAIN':
                    if 'domain' not in domain_rule:
                        domain_rule['domain'] = []
                    domain_rule['domain'].append(value)
                
                elif rule_type == 'DOMAIN-SUFFIX':
                    if 'domain_suffix' not in domain_rule:
                        domain_rule['domain_suffix'] = []
                    domain_rule['domain_suffix'].append(value if value.startswith('.') else f'.{value}')
                
                elif rule_type == 'DOMAIN-KEYWORD':
                    if 'domain_keyword' not in domain_rule:
                        domain_rule['domain_keyword'] = []
                    domain_rule['domain_keyword'].append(value)
                
                elif rule_type == 'IP-CIDR':
                    if 'ip_cidr' not in ip_rule:
                        ip_rule['ip_cidr'] = []
                    ip_rule['ip_cidr'].append(value)
        
        if domain_rule:
            singbox_rules.append(domain_rule)
        if ip_rule:
            singbox_rules.append(ip_rule)
        
        output = {
            "version": 1,
            "rules": singbox_rules
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ Clash –≤ Sing-Box: {output_file}")
    
    @staticmethod
    def v2ray_to_singbox(v2ray_file: str, output_file: str):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è V2Ray routing rules –≤ Sing-Box JSON"""
        with open(v2ray_file, 'r', encoding='utf-8') as f:
            v2ray_data = json.load(f)
        
        singbox_rules = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞ V2Ray
        if 'routing' in v2ray_data and 'rules' in v2ray_data['routing']:
            for rule in v2ray_data['routing']['rules']:
                domain_rule = {}
                ip_rule = {}
                
                if 'domain' in rule:
                    for domain in rule['domain']:
                        if domain.startswith('domain:'):
                            if 'domain_suffix' not in domain_rule:
                                domain_rule['domain_suffix'] = []
                            domain_rule['domain_suffix'].append(domain.replace('domain:', ''))
                        elif domain.startswith('full:'):
                            if 'domain' not in domain_rule:
                                domain_rule['domain'] = []
                            domain_rule['domain'].append(domain.replace('full:', ''))
                        elif domain.startswith('regexp:'):
                            if 'domain_regex' not in domain_rule:
                                domain_rule['domain_regex'] = []
                            domain_rule['domain_regex'].append(domain.replace('regexp:', ''))
                
                if 'ip' in rule:
                    ip_rule['ip_cidr'] = rule['ip']
                
                if domain_rule:
                    singbox_rules.append(domain_rule)
                if ip_rule:
                    singbox_rules.append(ip_rule)
        
        output = {
            "version": 1,
            "rules": singbox_rules
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∏–∑ V2Ray –≤ Sing-Box: {output_file}")

# ============================================================================
# –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï RULESET
# ============================================================================

class RulesetMerger:
    """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö ruleset –≤ –æ–¥–∏–Ω"""
    
    @staticmethod
    def merge_rulesets(input_files: List[str], output_file: str, deduplicate: bool = True):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ JSON ruleset –≤ –æ–¥–∏–Ω"""
        merged_rules = {
            'domain': [],
            'domain_suffix': [],
            'domain_keyword': [],
            'domain_regex': [],
            'ip_cidr': [],
            'source_ip_cidr': []
        }
        
        for input_file in input_files:
            try:
                with open(input_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if 'rules' not in data:
                    continue
                
                for rule in data['rules']:
                    for key in merged_rules.keys():
                        if key in rule:
                            merged_rules[key].extend(rule[key])
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {input_file}: {e}")
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        if deduplicate:
            for key in merged_rules:
                merged_rules[key] = list(set(merged_rules[key]))
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ ruleset
        output_rules = []
        
        domain_rule = {k: v for k, v in merged_rules.items() if k.startswith('domain') and v}
        if domain_rule:
            output_rules.append(domain_rule)
        
        ip_rule = {k: v for k, v in merged_rules.items() if 'ip' in k and v}
        if ip_rule:
            output_rules.append(ip_rule)
        
        output = {
            "version": 1,
            "rules": output_rules
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        total = sum(len(v) for v in merged_rules.values())
        print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(input_files)} —Ñ–∞–π–ª–æ–≤ –≤ {output_file}")
        print(f"üìä –í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {total}")
        
        return merged_rules

# ============================================================================
# –ê–ù–ê–õ–ò–ó RULESET
# ============================================================================

class RulesetAnalyzer:
    """–ê–Ω–∞–ª–∏–∑ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ruleset"""
    
    @staticmethod
    def analyze_ruleset(input_file: str):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ruleset"""
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        stats = {
            'total_rules': 0,
            'domain_count': 0,
            'domain_suffix_count': 0,
            'domain_keyword_count': 0,
            'domain_regex_count': 0,
            'ip_cidr_count': 0,
            'source_ip_cidr_count': 0,
            'top_tlds': Counter(),
            'avg_domain_length': 0,
            'duplicates': 0
        }
        
        all_items = []
        
        if 'rules' in data:
            for rule in data['rules']:
                for key, value in rule.items():
                    if isinstance(value, list):
                        stats[f'{key}_count'] += len(value)
                        all_items.extend(value)
                        
                        # –ê–Ω–∞–ª–∏–∑ TLD –¥–ª—è –¥–æ–º–µ–Ω–æ–≤
                        if 'domain' in key:
                            for domain in value:
                                if '.' in domain:
                                    tld = domain.split('.')[-1]
                                    stats['top_tlds'][tld] += 1
        
        # –ü–æ–¥—Å—á—ë—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        stats['duplicates'] = len(all_items) - len(set(all_items))
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–º–µ–Ω–æ–≤
        domain_items = [item for item in all_items if '.' in str(item)]
        if domain_items:
            stats['avg_domain_length'] = sum(len(d) for d in domain_items) / len(domain_items)
        
        stats['total_rules'] = len(all_items)
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\n" + "=" * 60)
        print(f"üìä –ê–ù–ê–õ–ò–ó RULESET: {os.path.basename(input_file)}")
        print("=" * 60)
        print(f"\nüìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  –í—Å–µ–≥–æ –ø—Ä–∞–≤–∏–ª: {stats['total_rules']}")
        print(f"  –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates']}")
        print(f"\nüåê –î–æ–º–µ–Ω—ã:")
        print(f"  –¢–æ—á–Ω—ã–µ –¥–æ–º–µ–Ω—ã: {stats['domain_count']}")
        print(f"  –°—É—Ñ—Ñ–∏–∫—Å—ã: {stats['domain_suffix_count']}")
        print(f"  –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {stats['domain_keyword_count']}")
        print(f"  Regex: {stats['domain_regex_count']}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {stats['avg_domain_length']:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"\nüî¢ IP –∞–¥—Ä–µ—Å–∞:")
        print(f"  IP CIDR: {stats['ip_cidr_count']}")
        print(f"  Source IP CIDR: {stats['source_ip_cidr_count']}")
        
        if stats['top_tlds']:
            print(f"\nüèÜ –¢–æ–ø-10 –¥–æ–º–µ–Ω–Ω—ã—Ö –∑–æ–Ω:")
            for tld, count in stats['top_tlds'].most_common(10):
                print(f"  .{tld}: {count}")
        
        print("=" * 60 + "\n")
        
        return stats
    
    @staticmethod
    def find_duplicates(input_file: str):
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ ruleset"""
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        all_items = []
        seen = set()
        duplicates = []
        
        if 'rules' in data:
            for rule in data['rules']:
                for key, value in rule.items():
                    if isinstance(value, list):
                        for item in value:
                            if item in seen:
                                duplicates.append((key, item))
                            else:
                                seen.add(item)
                            all_items.append(item)
        
        if duplicates:
            print(f"\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
            for key, item in duplicates[:20]:  # –ü–µ—Ä–≤—ã–µ 20
                print(f"  [{key}] {item}")
            if len(duplicates) > 20:
                print(f"  ... –∏ –µ—â—ë {len(duplicates) - 20}")
        else:
            print("‚úÖ –î—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        return duplicates
    
    @staticmethod
    def compare_rulesets(file1: str, file2: str):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤—É—Ö ruleset"""
        def extract_items(file):
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            items = set()
            if 'rules' in data:
                for rule in data['rules']:
                    for value in rule.values():
                        if isinstance(value, list):
                            items.update(value)
            return items
        
        items1 = extract_items(file1)
        items2 = extract_items(file2)
        
        only_in_1 = items1 - items2
        only_in_2 = items2 - items1
        common = items1 & items2
        
        print("\n" + "=" * 60)
        print(f"üîç –°–†–ê–í–ù–ï–ù–ò–ï RULESET")
        print("=" * 60)
        print(f"\nüìÑ –§–∞–π–ª 1: {os.path.basename(file1)}")
        print(f"  –ü—Ä–∞–≤–∏–ª: {len(items1)}")
        print(f"\nüìÑ –§–∞–π–ª 2: {os.path.basename(file2)}")
        print(f"  –ü—Ä–∞–≤–∏–ª: {len(items2)}")
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
        print(f"  –û–±—â–∏—Ö –ø—Ä–∞–≤–∏–ª: {len(common)} ({len(common)/max(len(items1), len(items2))*100:.1f}%)")
        print(f"  –¢–æ–ª—å–∫–æ –≤ —Ñ–∞–π–ª–µ 1: {len(only_in_1)}")
        print(f"  –¢–æ–ª—å–∫–æ –≤ —Ñ–∞–π–ª–µ 2: {len(only_in_2)}")
        
        if only_in_1 and len(only_in_1) <= 10:
            print(f"\n  –ü—Ä–∏–º–µ—Ä—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤ —Ñ–∞–π–ª–µ 1:")
            for item in list(only_in_1)[:10]:
                print(f"    ‚Ä¢ {item}")
        
        if only_in_2 and len(only_in_2) <= 10:
            print(f"\n  –ü—Ä–∏–º–µ—Ä—ã —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤ —Ñ–∞–π–ª–µ 2:")
            for item in list(only_in_2)[:10]:
                print(f"    ‚Ä¢ {item}")
        
        print("=" * 60 + "\n")

# ============================================================================
# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø RULESET
# ============================================================================

class RulesetOptimizer:
    """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ruleset –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞"""
    
    @staticmethod
    def optimize_domains(domains: List[str]) -> Dict[str, List[str]]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –¥–æ–º–µ–Ω–æ–≤"""
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ exact –∏ suffix
        exact_domains = set()
        suffix_domains = set()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–æ–º–µ–Ω—ã –ø–æ –¥–ª–∏–Ω–µ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
        sorted_domains = sorted(domains, key=len, reverse=True)
        
        for domain in sorted_domains:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ª–∏ —ç—Ç–æ—Ç –¥–æ–º–µ–Ω –∫–∞–∫–∏–º-—Ç–æ —Å—É—Ñ—Ñ–∏–∫—Å–æ–º
            covered = False
            for suffix in suffix_domains:
                if domain.endswith(suffix):
                    covered = True
                    break
            
            if not covered:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ —ç—Ç–æ—Ç –¥–æ–º–µ–Ω —Å–¥–µ–ª–∞—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º
                related = [d for d in sorted_domains if d.endswith(domain) and d != domain]
                if len(related) >= 2:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã 2 –ø–æ–¥–¥–æ–º–µ–Ω–∞
                    suffix_domains.add('.' + domain if not domain.startswith('.') else domain)
                else:
                    exact_domains.add(domain)
        
        return {
            'domain': list(exact_domains),
            'domain_suffix': list(suffix_domains)
        }
    
    @staticmethod
    def optimize_ruleset(input_file: str, output_file: str):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ruleset"""
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        optimized_rules = []
        
        if 'rules' in data:
            all_domains = []
            other_rules = {}
            
            for rule in data['rules']:
                for key, value in rule.items():
                    if key in ['domain', 'domain_suffix']:
                        all_domains.extend(value)
                    else:
                        if key not in other_rules:
                            other_rules[key] = []
                        other_rules[key].extend(value if isinstance(value, list) else [value])
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–æ–º–µ–Ω–æ–≤
            if all_domains:
                optimized = RulesetOptimizer.optimize_domains(all_domains)
                optimized_rules.append(optimized)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
            for key, value in other_rules.items():
                optimized_rules.append({key: list(set(value))})
        
        output = {
            "version": 1,
            "rules": optimized_rules
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        original_size = os.path.getsize(input_file)
        optimized_size = os.path.getsize(output_file)
        reduction = ((original_size - optimized_size) / original_size) * 100
        
        print(f"‚úÖ Ruleset –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω: {output_file}")
        print(f"üìâ –†–∞–∑–º–µ—Ä —É–º–µ–Ω—å—à–µ–Ω –Ω–∞ {reduction:.1f}%")
        print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {original_size} –±–∞–π—Ç")
        print(f"   –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {optimized_size} –±–∞–π—Ç")

# ============================================================================
# CLI
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ruleset",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest='command', help='–ö–æ–º–∞–Ω–¥—ã')
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
    convert_parser = subparsers.add_parser('convert', help='–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–æ–≤')
    convert_parser.add_argument('--from', dest='from_format', choices=['clash', 'v2ray'], required=True)
    convert_parser.add_argument('--input', required=True, help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    convert_parser.add_argument('--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    merge_parser = subparsers.add_parser('merge', help='–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ruleset')
    merge_parser.add_argument('--inputs', nargs='+', required=True, help='–í—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã')
    merge_parser.add_argument('--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    merge_parser.add_argument('--no-deduplicate', action='store_true', help='–ù–µ —É–¥–∞–ª—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã')
    
    # –ê–Ω–∞–ª–∏–∑
    analyze_parser = subparsers.add_parser('analyze', help='–ê–Ω–∞–ª–∏–∑ ruleset')
    analyze_parser.add_argument('--input', required=True, help='–§–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    analyze_parser.add_argument('--find-duplicates', action='store_true', help='–ò—Å–∫–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã')
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    compare_parser = subparsers.add_parser('compare', help='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ ruleset')
    compare_parser.add_argument('--file1', required=True, help='–ü–µ—Ä–≤—ã–π —Ñ–∞–π–ª')
    compare_parser.add_argument('--file2', required=True, help='–í—Ç–æ—Ä–æ–π —Ñ–∞–π–ª')
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    optimize_parser = subparsers.add_parser('optimize', help='–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ruleset')
    optimize_parser.add_argument('--input', required=True, help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    optimize_parser.add_argument('--output', required=True, help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    
    args = parser.parse_args()
    
    if args.command == 'convert':
        if args.from_format == 'clash':
            FormatConverter.clash_to_singbox(args.input, args.output)
        elif args.from_format == 'v2ray':
            FormatConverter.v2ray_to_singbox(args.input, args.output)
    
    elif args.command == 'merge':
        RulesetMerger.merge_rulesets(args.inputs, args.output, not args.no_deduplicate)
    
    elif args.command == 'analyze':
        RulesetAnalyzer.analyze_ruleset(args.input)
        if args.find_duplicates:
            RulesetAnalyzer.find_duplicates(args.input)
    
    elif args.command == 'compare':
        RulesetAnalyzer.compare_rulesets(args.file1, args.file2)
    
    elif args.command == 'optimize':
        RulesetOptimizer.optimize_ruleset(args.input, args.output)
    
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
